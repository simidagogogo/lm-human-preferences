"""
Byte pair encoding utilities
"""

import json
import os
from functools import lru_cache

import tensorflow as tf
import regex as re

# TODO
@lru_cache()
def bytes_to_unicode():
    """
    Returns list of utf-8 byte and a corresponding list of unicode strings.
    The reversible bpe codes work on unicode strings.
    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.
    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.
    This is a signficant percentage of your normal, say, 32K bpe vocab.
    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.
    And avoids mapping to whitespace/control characters the bpe code barfs on.
    """
    bs = list(range(ord("!"), ord("~") + 1)) + list(range(ord("Â¡"), ord("Â¬") + 1)) + list(range(ord("Â®"), ord("Ã¿") + 1))
    cs = bs[:]
    n = 0
    for b in range(2 ** 8):
        if b not in bs:
            bs.append(b)
            cs.append(2 ** 8 + n)
            n += 1
    cs = [chr(n) for n in cs]
    return dict(zip(bs, cs))


def get_pairs(word):
    """
    Return set of symbol pairs in a word.
    Word is represented as tuple of symbols (symbols being variable-length strings).
    è·å–å½“å‰wordä¸­æ‰€æœ‰ç›¸é‚»å­—ç¬¦å¯¹
    """
    pairs = set()
    prev_char = word[0]
    for char in word[1:]:
        pairs.add((prev_char, char))
        prev_char = char
    return pairs


class ReversibleEncoder:
    """
    å¯é€†åˆ†è¯å™¨ä¸ç¼–ç å™¨, ä¸»è¦ç”¨é€”:
    1. ç¼–ç encode: æ–‡æœ¬->tokenidåˆ—è¡¨
    2. è§£ç decode: tokenidåˆ—è¡¨->æ–‡æœ¬
    3. åŸºäºBPEç®—æ³•, å¹¶é«˜åº¦å…¼å®¹GPT-2/BPEåˆ†è¯è¡Œä¸º
    4. å¯é€†æ€§ã€å®¹é”™æ€§å¼º, å…·å¤‡padding/æœ«å°¾æ ‡è®°å¤„ç†
    """
    def __init__(self, encoder, bpe_merges, errors="replace", eot_token=None):
        """
        @encoder: unicodeå­—ç¬¦ä¸² -> tokenidæ˜ å°„
        @bpe_merges: è¯å¯¹åˆå¹¶è§„åˆ™, æ¯ä¸ªå…ƒç´ éƒ½æ˜¯ä¸€ä¸ªäºŒå…ƒç»„bigram, é…åˆBPEä½œç”¨
        @errors: è§£ç æ—¶å¦‚ä½•å¤„ç†é”™è¯¯(å¦‚"replace"/"ignore"/"strict")
        @eot_token: å¯é€‰. end-of-textç‰¹æ®Štokençš„id, ç”¨äºç”Ÿæˆç»ˆæ­¢ç¬¦
        """
        # unicodeå­—ç¬¦ä¸²->token_id
        self.encoder = encoder 
        
        # token_id->unicodeå­—ç¬¦ä¸²
        self.decoder = {v: k for k, v in self.encoder.items()}  
        
        # byte->unicode. å­—èŠ‚çº§ç¼–ç , ä¿è¯äºŒè¿›åˆ¶æ•°æ®å¯å®‰å…¨è½¬æˆæ–‡æœ¬.
        self.byte_encoder = bytes_to_unicode()
        
        # unicode->byte
        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}
        
        # how to handle errors in decoding
        self.errors = errors  
       
        # BPEåˆå¹¶è§„åˆ™çš„ä¼˜å…ˆçº§æ˜ å°„, rankè¶Šå°ä¼˜å…ˆçº§è¶Šé«˜
        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))
        self.eot_token = eot_token
        
        # ç¼“å­˜æ¯ä¸ªåˆ†è¯tokençš„BPEç»“æœ
        self.cache = {}
        
        # å¡«å……tokenid, ç”¨äºè¡¥é½åºåˆ—é•¿åº¦
        self.padding_token = len(encoder) + 2 # +2 unnecessary, for historical reasons
        
        # padding_tokençš„åå‘è§£ç ä¸ºç©ºå­—ç¬¦ä¸², ä¿è¯è§£ç æ—¶ä¸ä¼šæŠ¥é”™æˆ–å‡ºç°è„æ•°æ®
        self.decoder[self.padding_token] = ''

        # Should haved added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions
        # åº”è¯¥æ·»åŠ re.IGNORECASE, è¿™æ ·BPEåˆå¹¶æ“ä½œæ‰èƒ½å¯¹å¤§å†™å½¢å¼çš„ç¼©å†™ä¹Ÿç”Ÿæ•ˆ
        self.pat = re.compile(r"""'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+""")

    def bpe(self, token):
        """
        unicodeå­—ç¬¦ä¸²æ ¹æ®å·²çŸ¥bpe_mergesè§„åˆ™åå¤åˆå¹¶æˆæ›´ç²—çš„å­è¯å•å…ƒ, ç”¨äºå­è¯tokenization
        @token: unicodeå­—ç¬¦ä¸²
        @return: ç©ºæ ¼æ‹¼æ¥çš„unicodeå­—ç¬¦ä¸²
        
        æ­¥éª¤
        1. ç¼“å­˜å‘½ä¸­: å·²åˆ†å‰²è¿‡, ç›´æ¥æŸ¥è¡¨è¿”å›
        2. æ‹†æˆtuple: æ¯ä¸ªå­—ç¬¦æ‹†æˆå…ƒç»„å…ƒç´ , å¦‚'test'->('t','e','s','t')
        3. è·å–æ‰€æœ‰å¯åˆå¹¶pair
        4. å¾ªç¯: æ‰¾åˆ°æœ€ä¼˜å…ˆå¯åˆå¹¶çš„pair, æ ¹æ®bpe_ranksä¼˜å…ˆçº§åˆå¹¶å¹¶æ›´æ–°word. ç›´åˆ°å†æ— æ–°pairå¯åˆå¹¶æ—¶åœæ­¢
        5. æœ€åå°†æ‰€æœ‰åˆå¹¶ç»“æœæ‹¼æˆå­—ç¬¦ä¸²ï¼ŒåŠ å…¥ç¼“å­˜å¹¶è¿”å›
        è¿™ç§å¾ªç¯BPEæ–¹å¼å°±æ˜¯è®©tokenä»¥æœ€å°‘æ­¥éª¤åˆæˆæ•´ä¸ªè¯è¡¨ä¸­æ³¨å†Œè¿‡çš„å¤§token
        """
        # 1. ç¼“å­˜å‘½ä¸­
        if token in self.cache:
            return self.cache[token]
        
        # 2. æ‹†æˆtuple
        word = tuple(token)
        
        # 3. æ‰¾åˆ°æ‰€æœ‰å¯åˆå¹¶pair
        pairs = get_pairs(word)

        # å•å­—ç¬¦tokenç›´æ¥è¿”å›
        if not pairs:
            return token

        # 4. BPEæ ¸å¿ƒå¾ªç¯: åå¤åˆå¹¶ä¼˜å…ˆçº§æœ€é«˜çš„pair, ç›´åˆ°æ²¡æœ‰å¯åˆå¹¶pair
        while True:
            # 4.1 é€‰æ‹©å½“å‰æ‰€æœ‰pairä¸­â€œBPEä¼˜å…ˆçº§æœ€é«˜â€çš„bigram
            bigram = min(
                pairs, 
                key=lambda pair: self.bpe_ranks.get(pair, float("inf"))
            )
            
            # 4.2 å¦‚æœæœ€ä¼˜bigramæ²¡åœ¨BPEè§„åˆ™è¡¨é‡Œ, è¯´æ˜å·²ç»æ— æ³•ç»§ç»­åˆå¹¶äº†, é€€å‡ºå¾ªç¯
            if bigram not in self.bpe_ranks:
                break
            
            first, second = bigram
            new_word = []
            i = 0
            
            # 4.3 éå†å½“å‰çš„word, æ‰§è¡Œbigramåˆå¹¶
            while i < len(word):
                try:
                    # 4.3.1 æ‰¾åˆ°ä¸‹ä¸€ä¸ªfirstçš„ä½ç½®ï¼Œä»å½“å‰ä½ç½®iå¼€å§‹. jæ˜¯firstçš„ä¸‹æ ‡
                    j = word.index(first, i)
                    
                    # 4.3.2 å…ˆæŠŠiåˆ°jä¹‹é—´æ‰€æœ‰å­—ç¬¦åŠ å…¥new_word
                    new_word.extend(word[i:j])
                    i = j
                except:
                    # æ²¡æ‰¾åˆ°firstï¼Œç›´æ¥æŠŠå‰©ä¸‹çš„å­—ç¬¦å…¨éƒ¨åŠ å…¥new_wordï¼Œé€€å‡ºå†…å±‚å¾ªç¯
                    new_word.extend(word[i:])
                    break

                # 4.3.3 å¦‚æœfirståé¢æ­£å¥½è·Ÿç€secondï¼Œè¯´æ˜è¿™ä¸¤ä¸ªå¯ä»¥åˆå¹¶ï¼ˆ==è¦æŒ‰ç…§bigramåˆå¹¶åŸåˆ™åˆå¹¶ï¼‰
                if word[i] == first and i < len(word) - 1 and word[i + 1] == second:
                    new_word.append(first + second)
                    i += 2 # è·³è¿‡å·²åˆå¹¶çš„ä¸¤ä¸ªå­—ç¬¦
                else:
                    # ä¸æ˜¯bigramåŒ¹é…ï¼ŒæŒ‰åŸæ ·åŠ å…¥new_word
                    new_word.append(word[i])
                    i += 1

            # 4.4 æ›´æ–°word,å‡†å¤‡ä¸‹ä¸€è½®bpeåˆå¹¶. æ³¨æ„æ­¤æ—¶wordå·²æ¯”åŸæ¥çŸ­
            new_word = tuple(new_word)
            word = new_word
            
            # 4.5 å¦‚æœå˜æˆå•ä¸ªå…ƒç´ , è¯´æ˜å·²ç»æ— æ³•ç»§ç»­åˆå¹¶(æ˜¯è¯è¡¨ä¸­æœ€é•¿çš„token), é€€å‡ºå¾ªç¯
            if len(word) == 1:
                break
            else:
                # 4.6 å¦åˆ™é‡æ–°è®¡ç®—æ‰€æœ‰å¯åˆå¹¶pair, è¿›å…¥ä¸‹ä¸€è½®åˆå¹¶
                pairs = get_pairs(word)
        
        # 5. åˆå¹¶ç»“æŸ: ç”¨ç©ºæ ¼æ‹¼æ¥æ‰€æœ‰å­è¯, ä¾‹å¦‚()'he','llo') -> 'he llo')
        word = " ".join(word)
        
        # 6. å­˜å…¥ç¼“å­˜, æ–¹ä¾¿ä¸‹æ¬¡ç›´æ¥æŸ¥è¡¨, åŠ é€Ÿåˆ†è¯
        self.cache[token] = word
        return word


    def encode(self, text):
        """
        åŸå§‹æ–‡æœ¬ç¼–ç ä¸ºtokenid
        
        åœ¨GPTã€BERTç­‰æ¨¡å‹ä¸­, tokenizerå¸¸å¸¸å…ˆæŠŠå­—ç¬¦ä¸²ç¼–ç æˆUTF-8å­—èŠ‚
        è‹±æ–‡å­—ç¬¦ç”¨1ä¸ªå­—èŠ‚, ç‰¹æ®Šå­—ç¬¦å¯èƒ½ç”¨2~4å­—èŠ‚(ä¸­æ–‡ã€emoji)
        
        # å°†å­—ç¬¦ä¸²æŒ‰UTF-8ç¼–ç è½¬æ¢æˆå­—èŠ‚æµ
        s = "helloä¸–ç•ŒğŸ˜Š"
        b = s.encode("utf-8")  # å¾—åˆ°å­—èŠ‚æµ
        print(b) # è¾“å‡º: b'hello\xe4\xb8\x96\xe7\x95\x8c\xf0\x9f\x98\x8a' 
        """
        # print(f"text: {text}")
        bpe_token_ids = []
        for token in re.findall(self.pat, text):
            # åŸå§‹æ–‡æœ¬->å­—èŠ‚åºåˆ—->unicodeå­—ç¬¦ä¸². 
            # ä¿è¯æ‰€æœ‰tokenå‡ä¸ºå®‰å…¨æ˜¾ç¤ºå­—ç¬¦ä¸²,æ— è®ºåŸå§‹å†…å®¹æ˜¯æ–‡æœ¬è¿˜æ˜¯äºŒè¿›åˆ¶
            unicode_token = "".join(self.byte_encoder[b] for b in token.encode("utf-8"))
            bpe_tokens = [bpe_token for bpe_token in self.bpe(unicode_token).split(" ")]           
            tokens = [self.encoder[bpe_token] for bpe_token in bpe_tokens]
            # print(f"token: {token}, unicode_token: {unicode_token}, bpe_tokens: {bpe_tokens}, tokens: {tokens}")
            bpe_token_ids.extend(tokens)
        # print(f"len(bpe_token_ids): {len(bpe_token_ids)}, bpe_token_ids[:10]: {bpe_token_ids[:10]}")
        return bpe_token_ids
    """
    text: b w s r j y u p o o v. w e i t l g. e q m b j d n p.
    token: b, unicode_token: b, bpe_tokens: ['b'], tokens: [65]
    token:  w, unicode_token: Ä w, bpe_tokens: ['Ä w'], tokens: [266]
    token:  s, unicode_token: Ä s, bpe_tokens: ['Ä s'], tokens: [264]
    token:  r, unicode_token: Ä r, bpe_tokens: ['Ä r'], tokens: [374]
    token:  j, unicode_token: Ä j, bpe_tokens: ['Ä j'], tokens: [474]
    token:  y, unicode_token: Ä y, bpe_tokens: ['Ä y'], tokens: [331]
    token:  u, unicode_token: Ä u, bpe_tokens: ['Ä u'], tokens: [334]
    token:  p, unicode_token: Ä p, bpe_tokens: ['Ä p'], tokens: [279]
    token:  o, unicode_token: Ä o, bpe_tokens: ['Ä o'], tokens: [267]
    token:  o, unicode_token: Ä o, bpe_tokens: ['Ä o'], tokens: [267]
    token:  v, unicode_token: Ä v, bpe_tokens: ['Ä v'], tokens: [410]
    token: ., unicode_token: ., bpe_tokens: ['.'], tokens: [13]
    token:  w, unicode_token: Ä w, bpe_tokens: ['Ä w'], tokens: [266]
    token:  e, unicode_token: Ä e, bpe_tokens: ['Ä e'], tokens: [304]
    token:  i, unicode_token: Ä i, bpe_tokens: ['Ä i'], tokens: [1312]
    token:  t, unicode_token: Ä t, bpe_tokens: ['Ä t'], tokens: [256]
    token:  l, unicode_token: Ä l, bpe_tokens: ['Ä l'], tokens: [300]
    token:  g, unicode_token: Ä g, bpe_tokens: ['Ä g'], tokens: [308]
    token: ., unicode_token: ., bpe_tokens: ['.'], tokens: [13]
    token:  e, unicode_token: Ä e, bpe_tokens: ['Ä e'], tokens: [304]
    token:  q, unicode_token: Ä q, bpe_tokens: ['Ä q'], tokens: [10662]
    token:  m, unicode_token: Ä m, bpe_tokens: ['Ä m'], tokens: [285]
    token:  b, unicode_token: Ä b, bpe_tokens: ['Ä b'], tokens: [275]
    token:  j, unicode_token: Ä j, bpe_tokens: ['Ä j'], tokens: [474]
    token:  d, unicode_token: Ä d, bpe_tokens: ['Ä d'], tokens: [288]
    token:  n, unicode_token: Ä n, bpe_tokens: ['Ä n'], tokens: [299]
    token:  p, unicode_token: Ä p, bpe_tokens: ['Ä p'], tokens: [279]
    token: ., unicode_token: ., bpe_tokens: ['.'], tokens: [13]
    len(bpe_token_ids): 28, bpe_token_ids[:10]: [65, 266, 264, 374, 474, 331, 334, 279, 267, 267]
    begin tokens: [65, 266, 264, 374, 474, 331, 334, 279, 267, 267, 410, 13, 266, 304, 1312, 256, 300, 308, 13, 304, 10662, 285, 275, 474, 288, 299, 279, 13]
    final tokens: [266, 304, 1312, 256, 300, 308, 13, 304, 10662, 285, 275, 474, 288, 299, 279, 13, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259]
    """


    def decode(self, tokens, pretty=False):
        """
        tokenidè§£ç ä¸ºåŸå§‹æ–‡æœ¬
        
        ç›´è§‚æµç¨‹ä¸¾ä¾‹, å‡è®¾: 
            self.decoder = {15496: 'hello', 11: ' world', 0: '!'}
            tokens = [15496, 11, 0]
        1. "".join[self.decoder[token] for token in tokens] -> 'hello world!' # æ­¤å­—ç¬¦ä¸²æ˜¯ç»è¿‡byte_encoderçš„ç‰¹æ®Šå­—ç¬¦, è¿˜éœ€ä¸€æ­¥è¿˜åŸ
        2. self.byte_decoder[...] # æŠŠæ¯ä¸ªâ€œBPEç”Ÿæˆçš„ç‰¹æ®Šç¼–ç çš„å­—ç¬¦"è¿˜åŸæˆutf-8å­—èŠ‚
        3. bytearray(...).decode("utf-8") #å†ç”¨utf-8å…¨é¢è§£ç æˆæœ€ç»ˆäººç±»å¯è¯»æ–‡æœ¬
        
        ä¸ºä»€ä¹ˆè¿™ä¹ˆè®¾è®¡ï¼Ÿ
        æ”¯æŒä»»æ„äºŒè¿›åˆ¶-Token-æ–‡æœ¬æ— æŸè¿˜åŸ/æ¨¡å‹è¾“å…¥/è¾“å‡ºå’ŒçœŸå®æ–‡æœ¬/äºŒè¿›åˆ¶ä¹‹é—´ä¸ä¸¢ä»»ä½•ä¿¡æ¯(æ¯”å¦‚emojiã€æ±‰å­—ã€ç‰¹æ®Šç¬¦å·ç­‰)
        èƒ½å’Œencodeæ–¹æ³•å®Œå…¨å¯¹ç§°, æ— è®ºæ€ä¹ˆ encode, decode éƒ½èƒ½è¿˜åŸå›åŸæ–‡. æ•´ä¸ªç¼–ç -è§£ç è¿‡ç¨‹éƒ½æ˜¯å¯é€†å’Œä¿¡æ¯ä¸ä¸¢å¤±çš„ï¼
        æ”¯æŒå„ç±»æ¨¡å‹, ä¾‹å¦‚GPT-2/3ã€T5ç­‰, å®é™…å°±æ˜¯åœ¨åšâ€œtokenizerçš„å¯é€†æ˜ å°„â€
        """
        del pretty
        
        # tokenid->unicodeå­—ç¬¦ä¸². å¤‡æ³¨:BPEåˆ†è¯ç»“æœå¯èƒ½ä¼šæœ‰å‰å¯¼ç©ºæ ¼
        text = "".join([self.decoder[token] for token in tokens])
        print(f"tokens: {tokens}")
        print(f"unicode text: {text}")
        
        # unicodeå­—ç¬¦ä¸²->byteåºåˆ—->åŸå§‹æ–‡æœ¬
        text = bytearray([self.byte_decoder[c] for c in text]).decode("utf-8", errors=self.errors)
        print(f"final text: {text}")
        
        """
        tokens: [19638, 38271, 37400, 15425, 45738, 43003, 7420, 47041, 7291, 19744, 44978, 41022, 15421, 23106, 39383, 14100, 7453, 7780, 32145, 40536, 19163, 18020, 20907, 18273]
        unicode text: Ä packetaucusesÄ NingÄ villagesÄ FrankensteinÄ canineÄ SaudibahÄ facilitiesÄ pricedanskiÄ MbpsÄ traditionsHeightÄ dedicatemillionÄ eligermÄ DoddÄ ComplianceÄ appreciationÄ CaribbeanÄ PTSUntil
        final text: packetaucuses Ning villages Frankenstein canine Saudibah facilities pricedanski Mbps traditionsHeight dedicatemillion eligerm Dodd Compliance appreciation Caribbean PTSUntil
        packetaucuses Ning villages Frankenstein canine Saudibah facilities pricedanski Mbps traditionsHeight dedicatemillion eligerm Dodd Compliance appreciation Caribbean PTSUntil        
        """
        return text


def read_file(path):
    """
    ä»¥äºŒè¿›åˆ¶æ–¹å¼è¯»å–æ–‡ä»¶å†…å®¹
    """
    with tf.gfile.Open(path, "rb") as fh:
        return fh.read()


class Encoding:
    def __init__(
        self,
        name,
        *,
        n_vocab=0,
        eot_token=None,
        encoder_path="encoder.json",
        bpe_path="vocab.bpe",
        base_path=None,
    ):
        self.name = name
        self.eot_token = eot_token
        self.n_vocab = n_vocab

        """
         ~/PycharmProjects/lm-human-preferences master Â± tree ~/gpt-2-models/encodings 
        /Users/xiangqian/gpt-2-models/encodings
        â””â”€â”€ main
            â”œâ”€â”€ encoder.json
            â””â”€â”€ vocab.bpe
        """
        if base_path is None:
            local_base = os.environ.get('GPT2_MODEL_PATH', os.path.expanduser('~/gpt-2-models'))
            local_encoding_path = os.path.join(local_base, 'encodings', name)
            if os.path.exists(os.path.join(local_encoding_path, encoder_path)):
                base_path = local_encoding_path
            else:
                base_path = os.path.join("gs://gpt-2/encodings", name) # å›é€€åˆ° GCS è·¯å¾„

        self.base_path = base_path
        print(f"name: {name}, self.base_path: {self.base_path}")
        if name != "test":
            self.encoder_path = os.path.join(self.base_path, encoder_path)
            self.bpe_path = os.path.join(self.base_path, bpe_path)
            print(f"name: {name}, self.encoder_path: {self.encoder_path}")
            print(f"name: {name}, self.bpe_path: {self.bpe_path}")
        """
        name: main, self.base_path: /root/gpt-2-models/encodings/main
        name: main, self.encoder_path: /root/gpt-2-models/encodings/main/encoder.json
        name: main, self.bpe_path: /root/gpt-2-models/encodings/main/vocab.bpe
        name: test, self.base_path: gs://gpt-2/encodings/test
        """

    def get_encoder(self):
        if self.name == "test":
            vocab = "abcdefghijklmnopqrstuvwxyz."
            assert len(vocab) == self.n_vocab
            class TestEncoder(ReversibleEncoder):
                def __init__(self):
                    super().__init__(encoder={w: i for i, w in enumerate(vocab)}, bpe_merges=list())
                    self.padding_token = len(vocab)
                def encode(self, text):
                    return [self.encoder.get(x, len(vocab) - 1) for x in text]
                def decode(self, tokens, pretty=False):
                    return ''.join([self.decoder.get(t, '<unk>') for t in tokens])
            return TestEncoder()

        """
        å…¸å‹å·¥ä½œæµ
        1. å§‹ç»ˆç”¨"rb"äºŒè¿›åˆ¶è¯»å–æ–‡ä»¶, è·å¾—æœ€åŸå§‹çš„æ•°æ®
        2. å¦‚æœç¡®å®šæ˜¯æ–‡æœ¬å†ç”¨æ­£ç¡®çš„.decode()æ˜ç¡®å®šåˆ¶ç¼–ç 
            ä¾‹å¦‚ .decode("utf-8")ã€.decode("gbk") ã€.decode(errors="ignore") ç­‰
        3. è¿™æ ·çš„æ¨¡å¼æ˜¯é«˜è´¨é‡NLPå·¥ç¨‹å’Œæ·±åº¦å­¦ä¹ å·¥ç¨‹çš„æ ‡å‡†å®è·µã€‚
        
        # ç›´æ¥ç”¨æ–‡æœ¬æ–¹å¼æ‰“å¼€, æœ‰é£é™©
        with open("xxx.json", "r") as f:
            data = f.read()  # å¦‚æœè¯¥æ–‡ä»¶ä¸æ˜¯UTF-8æˆ–è€…æœ‰éæ ‡å‡†å­—ç¬¦, å®¹æ˜“æŠ¥é”™ã€å†…å®¹è¯»é”™

        # æ¨èæ–¹å¼
        with open("xxx.json", "rb") as f:
            data = f.read()      # å¾—åˆ°bytes, æœ€å®‰å…¨
            text = data.decode() # æ˜ç¡®æŒ‡å®šç¼–ç å¦‚utf-8
        """
        # unicodeå­—ç¬¦ä¸²->token_id. ä¾‹å¦‚{"\u0120gazed": 50255, "<|endoftext|>": 50256}
        encoder_dict = json.loads(read_file(self.encoder_path).decode())
        assert len(encoder_dict) == self.n_vocab
        
        # BPEä¸­å­—ç¬¦mergeè§„åˆ™. ä¾‹å¦‚[("Ä ", "t"), ("Ä ", "a")]
        bpe_data = read_file(self.bpe_path).decode()        
        bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split("\n")[1:-1]]
        
        print(f"len(encoder_dict): {len(encoder_dict)}")
        print(f"encoder_dict[:5]: {dict(list(encoder_dict.items())[:5])}")
        print(f"len(bpe_merges): {len(bpe_merges)}")
        print(f"bpe_merges[:5]: {bpe_merges[:5]}")
        # len(encoder_dict): 50257
        # encoder_dict[:5]: {'!': 0, '"': 1, '#': 2, '$': 3, '%': 4}
        # len(bpe_merges): 50000
        # bpe_merges[:5]: [('Ä ', 't'), ('Ä ', 'a'), ('h', 'e'), ('i', 'n'), ('r', 'e')]
        
        encoder = ReversibleEncoder(
            encoder=encoder_dict, 
            bpe_merges=bpe_merges, 
            eot_token=self.eot_token
        )
        assert encoder.padding_token >= self.n_vocab
        return encoder


Main = Encoding("main", n_vocab=50257, eot_token=50256)
Test = Encoding("test", n_vocab=27, eot_token=26)
